# Unsupervised Concept Drift Detection - Learning Fork

> ğŸ”— **Original Repository**: [DFKI-NI/unsupervised-concept-drift-detection](https://github.com/DFKI-NI/unsupervised-concept-drift-detection)
>
> This is my personal fork for learning and running experiments with unsupervised concept drift detection algorithms.

---

## ğŸ“š Table of Contents

1. [What is Concept Drift?](#-what-is-concept-drift)
2. [Project Overview](#-project-overview)
3. [How the Code Works](#-how-the-code-works)
4. [Directory Structure](#-directory-structure)
5. [Installation Guide](#-installation-guide)
6. [Dataset Setup](#-dataset-setup)
7. [Running the Project](#-running-the-project)
8. [Understanding the Detectors](#-understanding-the-detectors)
9. [Understanding the Metrics](#-understanding-the-metrics)
10. [Dataset Fix Documentation](#-dataset-fix-documentation)
11. [Test Results](#-test-results)

---

## ğŸ“ What is Concept Drift?

**Concept drift** occurs when the statistical properties of data change over time. Imagine:

- A spam filter trained on 2020 emails fails in 2025 because spam patterns evolved
- A weather prediction model trained on summer data performs poorly in winter
- A fraud detection system misses new types of fraud it wasn't trained on

**Unsupervised concept drift detection** finds these changes **without needing labels** - it only looks at the features (X), not the target (y).

### Why Does It Matter?

If left undetected, concept drift makes machine learning models unreliable. By detecting drift, we can:
- Retrain models when needed
- Alert operators to investigate changes
- Maintain prediction accuracy over time

---

## ğŸ”­ Project Overview

This repository benchmarks **10 unsupervised concept drift detectors** on **real-world data streams**:

| Detector | Full Name | Key Idea |
|----------|-----------|----------|
| **BNDM** | Bayesian Non-parametric Detection Method | Uses Bayesian statistics to detect distribution changes |
| **CSDDM** | Clustered Statistical Test DDM | Clusters data and uses statistical tests |
| **D3** | Discriminative Drift Detector | Trains classifier to distinguish old vs new data |
| **EDFS** | Ensemble Drift with Feature Subspaces | Uses ensemble of detectors on feature subsets |
| **IBDD** | Image-Based Drift Detector | Converts data to images and detects visual changes |
| **NN-DVI** | Nearest Neighbor Density Variation | Measures density changes using nearest neighbors |
| **OCDD** | One-Class Drift Detector | Uses one-class classification |
| **SPLL** | Semi-Parametric Log Likelihood | Measures likelihood ratio changes |
| **UCDD** | Unsupervised Concept Drift Detection | Uses clustering-based approach |
| **UDetect** | Unsupervised Change Detection | Activity recognition approach |

---

## ğŸ”„ How the Code Works

### High-Level Flow

```
main.py
   â”‚
   â–¼
runner.py â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚                                               â”‚
   â–¼                                               â”‚
config.py (defines which datasets & detectors)    â”‚
   â”‚                                               â”‚
   â–¼                                               â”‚
For each (dataset, detector) combination:         â”‚
   â”‚                                               â”‚
   â–¼                                               â”‚
ModelOptimizer.optimize()                         â”‚
   â”‚                                               â”‚
   â”œâ”€â”€â–º Stream data sample-by-sample              â”‚
   â”‚       â”‚                                       â”‚
   â”‚       â–¼                                       â”‚
   â”‚    detector.update(features) â”€â”€â–º Returns True if drift detected
   â”‚       â”‚                                       â”‚
   â”‚       â–¼                                       â”‚
   â”‚    If drift: Reset classifiers              â”‚
   â”‚       â”‚                                       â”‚
   â”‚       â–¼                                       â”‚
   â”‚    Train classifiers on sample              â”‚
   â”‚                                               â”‚
   â–¼                                               â”‚
Calculate metrics (accuracy, LPD, MTR, etc.)      â”‚
   â”‚                                               â”‚
   â–¼                                               â”‚
Save results to CSV â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Components Explained

#### 1. **Detector Base Class** (`detectors/base.py`)
```python
class UnsupervisedDriftDetector(ABC):
    def update(self, features: dict) -> bool:
        """
        Feed one sample to the detector.
        Returns True if drift is detected, False otherwise.
        """
```

All detectors implement this interface. You feed samples one by one, and the detector signals when it thinks the data distribution has changed.

#### 2. **Model Optimizer** (`optimization/model_optimizer.py`)
This is the experiment runner. For each detector configuration:
1. Streams data sample-by-sample
2. Calls `detector.update()` for each sample
3. If drift detected: resets the "assisted" classifiers
4. Trains classifiers on each sample
5. Records metrics at the end

#### 3. **Classifiers** (`optimization/classifiers.py`)
Maintains 4 classifiers to evaluate detector quality:
- **Base Hoeffding Tree** - Never reset, ignores drift signals
- **Base Naive Bayes** - Never reset, ignores drift signals
- **Assisted Hoeffding Tree** - Reset when detector signals drift
- **Assisted Naive Bayes** - Reset when detector signals drift

If the **assisted** classifiers perform better, the detector is helpful!

---

## ğŸ“ Directory Structure

```
unsupervised-concept-drift-detection/
â”‚
â”œâ”€â”€ main.py                 # Entry point - starts experiments
â”œâ”€â”€ runner.py               # Runs all detector/dataset combinations
â”œâ”€â”€ config.py               # Configuration: which datasets & detectors to test
â”œâ”€â”€ demo.py                 # â­ Simple demo showing drift detection step-by-step
â”œâ”€â”€ add_headers.py          # â­ Helper script to add headers to USP DS CSVs
â”œâ”€â”€ convert_datasets.py     # Original script to convert .arff to .csv
â”œâ”€â”€ eval.py                 # Evaluation and plotting script
â”œâ”€â”€ requirements.txt        # Python dependencies
â”‚
â”œâ”€â”€ datasets/               # Dataset loader classes
â”‚   â”œâ”€â”€ __init__.py         # Exports all dataset classes
â”‚   â”œâ”€â”€ insects.py          # INSECTS datasets (10 variants)
â”‚   â”œâ”€â”€ electricity.py      # Electricity price dataset
â”‚   â”œâ”€â”€ noaa_weather.py     # NOAA weather dataset
â”‚   â”œâ”€â”€ outdoor_objects.py  # Outdoor objects dataset
â”‚   â”œâ”€â”€ poker_hand.py       # Poker hand dataset
â”‚   â”œâ”€â”€ ...                 # Other dataset loaders
â”‚   â””â”€â”€ files/              # âš ï¸ Put CSV data files here
â”‚       â””â”€â”€ .gitkeep
â”‚
â”œâ”€â”€ detectors/              # Drift detection algorithms
â”‚   â”œâ”€â”€ __init__.py         # Exports all detector classes
â”‚   â”œâ”€â”€ base.py             # Abstract base class
â”‚   â”œâ”€â”€ d3.py               # D3 - Discriminative Drift Detector
â”‚   â”œâ”€â”€ ibdd.py             # IBDD - Image-Based Drift Detector
â”‚   â”œâ”€â”€ spll.py             # SPLL - Semi-Parametric Log Likelihood
â”‚   â”œâ”€â”€ ...                 # Other detectors
â”‚
â”œâ”€â”€ metrics/                # Performance measurement
â”‚   â”œâ”€â”€ metrics.py          # Main metrics calculation
â”‚   â”œâ”€â”€ drift.py            # MTR, MTFA, MTD, MDR calculations
â”‚   â””â”€â”€ lift_per_drift.py   # LPD calculation
â”‚
â”œâ”€â”€ optimization/           # Experiment infrastructure
â”‚   â”œâ”€â”€ model_optimizer.py  # Main experiment runner
â”‚   â”œâ”€â”€ classifiers.py      # HoeffdingTree & NaiveBayes classifiers
â”‚   â”œâ”€â”€ config_generator.py # Generates parameter combinations
â”‚   â”œâ”€â”€ logger.py           # Saves results to CSV
â”‚   â””â”€â”€ parameter.py        # Parameter range definitions
â”‚
â”œâ”€â”€ eval/                   # Result analysis
â”‚   â”œâ”€â”€ cleaner.py          # Cleans result files
â”‚   â”œâ”€â”€ plotter.py          # Generates plots
â”‚   â”œâ”€â”€ summarize.py        # Summarizes results
â”‚   â””â”€â”€ parser.py           # Parses result files
â”‚
â”œâ”€â”€ results/                # Raw experiment results (CSV files)
â”‚   â”œâ”€â”€ Elec2/
â”‚   â”œâ”€â”€ InsectsAbruptBalanced/
â”‚   â””â”€â”€ ...
â”‚
â””â”€â”€ test/                   # Unit tests
    â”œâ”€â”€ datasets/           # Dataset tests
    â”œâ”€â”€ detectors/          # Detector tests
    â”œâ”€â”€ metrics/            # Metrics tests
    â”œâ”€â”€ optimization/       # Optimization tests
    â””â”€â”€ integration/        # Integration tests
```

---

## ğŸ›  Installation Guide

### Prerequisites
- Python 3.8+ (tested with 3.13)
- pip (Python package manager)
- Git

### Step 1: Clone the Repository

```bash
git clone https://github.com/NusratBegum/unsupervised-concept-drift-detection.git
cd unsupervised-concept-drift-detection
```

### Step 2: Create Virtual Environment

```bash
# Create virtual environment
python3 -m venv venv

# Activate it
source venv/bin/activate  # On macOS/Linux
# or
.\venv\Scripts\activate   # On Windows
```

### Step 3: Install Dependencies

```bash
pip install --upgrade pip
pip install -r requirements.txt
```

### Step 4: Verify Installation

```bash
# Run a quick test (works without datasets)
python -m unittest test.detectors.test_d3
```

Expected output:
```
.
----------------------------------------------------------------------
Ran 1 test in 0.012s

OK
```

---

## ğŸ“Š Dataset Setup

### About the USP DS Repository

The datasets come from the **USP Data Stream Repository** maintained by researchers at University of SÃ£o Paulo. The repository contains real-world data streams with known concept drift points.

**Download Link**: [USP DS Repository](https://sites.google.com/view/uspdsrepository)

> âš ï¸ **Note**: The archive is password-protected. The password is provided in the paper:
> *"Challenges in Benchmarking Stream Learning Algorithms with Real-world Data"* by Souza et al.

### Step-by-Step Dataset Setup

#### 1. Download the Dataset Archive
- Go to [USP DS Repository](https://sites.google.com/view/uspdsrepository)
- Download the dataset archive
- Extract using the password from the paper

#### 2. Copy Files to the Project
Place the extracted `USP DS Repository` folder inside `datasets/files/`:

```
datasets/files/
â””â”€â”€ USP DS Repository/
    â”œâ”€â”€ INSECTS/
    â”‚   â”œâ”€â”€ INSECTS abrupt_balanced.csv
    â”‚   â”œâ”€â”€ INSECTS gradual_balanced.csv
    â”‚   â””â”€â”€ ...
    â”œâ”€â”€ Old datasets/
    â”‚   â”œâ”€â”€ NOAA.csv
    â”‚   â”œâ”€â”€ Outdoor.csv
    â”‚   â”œâ”€â”€ Electricity.csv
    â”‚   â””â”€â”€ ...
    â””â”€â”€ New datasets/
        â””â”€â”€ ...
```

#### 3. Run the Header Script
The USP DS Repository CSV files **don't have headers**. Run this script to:
- Copy files to the correct location with correct names
- Add proper header rows

```bash
python add_headers.py
```

Expected output:
```
Adding headers to USP DS Repository CSV files...

  INSECTS-abrupt_balanced_norm.csv: Added header
  INSECTS-abrupt_imbalanced_norm.csv: Added header
  ...
  NOAA.csv: Added header
  outdoor.csv: Added header
  ...

Done! You can now run the tests.
```

#### 4. Verify Dataset Setup

```bash
# Run all tests - should show 105 tests passing
python -m unittest discover -s test -t .
```

Expected output:
```
....................................s...........................................
.........................
----------------------------------------------------------------------
Ran 105 tests in 11.050s

OK (skipped=1)
```

---

## ğŸš€ Running the Project

### Option 1: Run the Demo (Recommended for Learning)

```bash
python demo.py
```

This shows drift detection step-by-step with explanations:

```
============================================================
STEP 1: Loading the INSECTS Abrupt Balanced dataset
============================================================
Dataset: InsectsAbruptBalanced
Number of samples: 52,848
Number of features: 33
Known drift points: [14352, 19500, 33240, 38682, 39510]

============================================================
STEP 2: Initializing the D3 (Discriminative Drift Detector)
============================================================
...

============================================================
STEP 3: Processing the data stream...
============================================================
  Processed 10,000 samples...
  ğŸ”´ DRIFT DETECTED at sample 14,439
  ...

============================================================
STEP 4: Results - Comparing detected vs actual drifts
============================================================
Actual drift points: [14352, 19500, 33240, 38682, 39510]
Detected drift points: [14439, 19588, 33221, 38773, ...]

Analysis:
  âœ… Drift at 14,352 detected at 14,439 (delay: +87)
  âœ… Drift at 19,500 detected at 19,588 (delay: +88)
  ...
```

### Option 2: Run Tests

```bash
# Run all tests
python -m unittest discover -s test -t .

# Run specific test suites
python -m unittest test.detectors           # All detector tests
python -m unittest test.datasets.test_insects  # INSECTS tests
python -m unittest test.metrics             # Metrics tests
```

### Option 3: Run Full Experiments

```bash
# Run full experiment suite (takes a long time!)
python main.py my_experiment

# With limited threads
OMP_NUM_THREADS=4 python main.py my_experiment
```

Results are saved to `results/<dataset>/<detector>_my_experiment.csv`

### Option 4: Evaluate Results

```bash
python eval.py
```

This generates:
- Summary statistics
- Plots and figures
- Best configuration rankings

---

## ğŸ”¬ Understanding the Detectors

### Example: D3 (Discriminative Drift Detector)

**Location**: `detectors/d3.py`

**How it works**:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    D3 Detection Process                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  1. Collect reference samples (old data window)             â”‚
â”‚     â”Œâ”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”                      â”‚
â”‚     â”‚ â— â”‚ â— â”‚ â— â”‚ â— â”‚ â— â”‚ â— â”‚ â— â”‚ â— â”‚  â† 200 samples       â”‚
â”‚     â””â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”˜                      â”‚
â”‚                                                             â”‚
â”‚  2. Collect recent samples (new data window)                â”‚
â”‚                         â”Œâ”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”                  â”‚
â”‚                         â”‚ â—‹ â”‚ â—‹ â”‚ â—‹ â”‚ â—‹ â”‚  â† 100 samples   â”‚
â”‚                         â””â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”˜                  â”‚
â”‚                                                             â”‚
â”‚  3. Label them: reference=0, recent=1                       â”‚
â”‚                                                             â”‚
â”‚  4. Train a classifier to distinguish them                  â”‚
â”‚     (Logistic Regression)                                   â”‚
â”‚                                                             â”‚
â”‚  5. Calculate AUC score                                     â”‚
â”‚     - AUC â‰ˆ 0.5 â†’ Can't distinguish â†’ NO DRIFT             â”‚
â”‚     - AUC > 0.7 â†’ Can distinguish â†’ DRIFT DETECTED!        â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Intuition**: If a classifier can tell old data from new data, they must be different!

### Detector Parameters

Each detector has tunable parameters. Example for D3:

```python
detector = DiscriminativeDriftDetector2019(
    n_reference_samples=200,       # Size of "old" data window
    recent_samples_proportion=0.5, # Size of "new" window (relative)
    threshold=0.7,                 # AUC threshold for detection
    seed=42                        # Random seed for reproducibility
)
```

---

## ğŸ“ˆ Understanding the Metrics

### Key Metrics

| Metric | Full Name | What It Measures |
|--------|-----------|------------------|
| **acc (ht-dd)** | Accuracy (Hoeffding Tree with Drift Detector) | Classification accuracy with drift detection |
| **acc (ht-no dd)** | Accuracy (Hoeffding Tree, no Drift Detector) | Baseline accuracy without drift detection |
| **lpd (ht)** | Lift Per Drift | Accuracy improvement per detected drift |
| **mtr** | Mean Time to Reaction | Average delay to detect a drift |
| **mtfa** | Mean Time between False Alarms | How often false alarms occur |
| **mtd** | Mean Time to Detection | Time from drift to detection |
| **mdr** | Missed Detection Rate | Proportion of drifts not detected |

### Interpretation

- **Higher is better**: `acc`, `lpd`, `mtfa`
- **Lower is better**: `mtr`, `mtd`, `mdr`

### Example Results

```
Actual drift at: 14,352
Detected at:     14,439
Delay (MTR):     +87 samples

If delay is small â†’ Good detection!
If delay is large or negative â†’ Poor detection
```

---

## ğŸ”§ Dataset Fix Documentation

### The Problem

The original code was written for `.arff` files (ARFF format includes headers). The USP DS Repository provides `.csv` files **without headers**.

### Issues Found

| Issue | Description |
|-------|-------------|
| **Missing headers** | CSV files from USP DS have raw data only, no column names |
| **Different column names** | Code expected specific names like `Att1, Att2, ...` |
| **Format mismatch** | Some loaders expected `.arff`, USP DS has `.csv` |
| **Sample count differences** | USP DS versions have different row counts |

### Fixes Applied

#### 1. Created `add_headers.py`
This script adds proper headers to all CSV files:

```python
# Example of what it does:
# Before: 19.8,14,1019.6,8.4,9.9,15.9,28.9,14,1
# After:  attribute1,attribute2,...,class (header row)
#         19.8,14,1019.6,8.4,9.9,15.9,28.9,14,1
```

#### 2. Updated Dataset Loaders
Modified these files to use CSV format:

| File | Change |
|------|--------|
| `datasets/airlines.py` | Use CSV, fix string column types |
| `datasets/chess.py` | Use CSV, update column names to `at1-at8` |
| `datasets/electricity.py` | Use CSV instead of ARFF |
| `datasets/intrusion_detection.py` | Use CSV instead of ARFF |
| `datasets/keystroke.py` | Use CSV instead of ARFF |

#### 3. Updated `requirements.txt`
Changed from exact versions (`==`) to minimum versions (`>=`) for Python 3.13 compatibility:

```
matplotlib>=3.6.3
numpy>=1.23.1
pandas>=1.4.3
river>=0.11.1
scipy>=1.8.1
scikit-learn>=1.1.1
```

---

## âœ… Test Results

### Current Status

| Test Suite | Tests | Status |
|------------|-------|--------|
| Detectors | 34 | âœ… All pass |
| Metrics | 8 | âœ… All pass |
| Optimization | 34 | âœ… All pass |
| Integration | 3 | âœ… All pass |
| Datasets | 26 | âœ… All pass (1 skipped) |
| **Total** | **105** | âœ… **All pass** |

### Working Datasets

| Dataset | Samples | Features | Has Ground Truth Drifts |
|---------|---------|----------|------------------------|
| INSECTS Abrupt Balanced | 52,848 | 33 | âœ… Yes |
| INSECTS Gradual Balanced | 24,150 | 33 | âœ… Yes |
| INSECTS Incremental Balanced | 57,018 | 33 | âœ… Yes |
| INSECTS Incremental-Abrupt Balanced | 79,986 | 33 | âœ… Yes |
| INSECTS Incremental-Reoccurring Balanced | 79,986 | 33 | âœ… Yes |
| NOAA Weather | 18,159 | 8 | âŒ No |
| Outdoor Objects | 4,000 | 21 | âŒ No |
| Electricity | 45,312 | 8 | âŒ No |
| Poker Hand | 829,201 | 10 | âŒ No |
| Powersupply | 29,928 | 2 | âŒ No |
| Sensor Stream | 2,219,803 | 5 | âŒ No |
| And more... | | | |

---

## ğŸ“ Quick Reference

### Common Commands

```bash
# Activate virtual environment
source venv/bin/activate

# Run demo
python demo.py

# Run all tests
python -m unittest discover -s test -t .

# Run specific detector test
python -m unittest test.detectors.test_d3

# Run experiment
python main.py experiment_name

# Evaluate results
python eval.py
```

### File Locations

| What | Where |
|------|-------|
| Dataset CSV files | `datasets/files/*.csv` |
| Experiment results | `results/<dataset>/<detector>.csv` |
| Demo script | `demo.py` |
| Header fixer | `add_headers.py` |

---

## ğŸ”— References

- Original Paper: [A benchmark and survey of fully unsupervised concept drift detectors](https://link.springer.com/article/10.1007/s41060-024-00620-y)
- Original Repository: [DFKI-NI/unsupervised-concept-drift-detection](https://github.com/DFKI-NI/unsupervised-concept-drift-detection)
- USP DS Repository: [sites.google.com/view/uspdsrepository](https://sites.google.com/view/uspdsrepository)

---

## ğŸ“„ License

BSD 3-Clause License - See [LICENSE](LICENSE) file.
