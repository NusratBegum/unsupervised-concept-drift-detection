{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4eb65fbe",
   "metadata": {},
   "source": [
    "# EADD Thesis Experiments: Feature Drift Detection via Adversarial Validation\n",
    "\n",
    "**Author:** Nusrat Begum  \n",
    "**Institution:** Mahidol University, Faculty of ICT  \n",
    "**Thesis:** Feature Drift Detection via Adversarial Validation  \n",
    "**Benchmark Reference:** Lukats, Zielinski, Hahn & Stahl (2024) — *A benchmark and survey of fully unsupervised concept drift detectors on real-world data streams*\n",
    "\n",
    "---\n",
    "\n",
    "This notebook conducts the complete experimental evaluation for the EADD (Explainable Adversarial Drift Detection) framework, following the benchmark methodology from Lukats et al. (2024). It covers:\n",
    "\n",
    "1. Synthetic and real-world dataset evaluation  \n",
    "2. Detection performance metrics (F1, precision, recall, latency)  \n",
    "3. Explainability via SHAP feature attribution  \n",
    "4. Statistical hypothesis testing (Wilcoxon, Friedman, Nemenyi)  \n",
    "5. Publication-ready figures and LaTeX tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5ced20",
   "metadata": {},
   "source": [
    "## Section 1: Import Libraries and Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdbdcc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from collections import defaultdict\n",
    "\n",
    "# ML\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, roc_curve\n",
    "import lightgbm as lgb\n",
    "import shap\n",
    "\n",
    "# Set project root\n",
    "PROJECT_ROOT = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "sys.path.insert(0, PROJECT_ROOT)\n",
    "\n",
    "# Import project modules\n",
    "from detectors.eadd import ExplainableAdversarialDriftDetector\n",
    "from detectors.d3 import D3\n",
    "try:\n",
    "    from detectors.ibdd import IBDD\n",
    "except ImportError:\n",
    "    IBDD = None\n",
    "try:\n",
    "    from detectors.ocdd import OCDD\n",
    "except ImportError:\n",
    "    OCDD = None\n",
    "try:\n",
    "    from detectors.spll import SPLL\n",
    "except ImportError:\n",
    "    SPLL = None\n",
    "try:\n",
    "    from detectors.bndm import BNDM\n",
    "except ImportError:\n",
    "    BNDM = None\n",
    "try:\n",
    "    from detectors.csddm import CSDDM\n",
    "except ImportError:\n",
    "    CSDDM = None\n",
    "try:\n",
    "    from detectors.udetect import UDetect\n",
    "except ImportError:\n",
    "    UDetect = None\n",
    "\n",
    "# Import datasets\n",
    "from datasets import sine_clusters, waveform_drift2\n",
    "from datasets import electricity, insects, noaa_weather, powersupply\n",
    "from datasets import luxembourg, outdoor_objects, ozone, poker_hand\n",
    "from datasets import rialto_bridge_timelapse\n",
    "\n",
    "# Import metrics\n",
    "from metrics.drift import compute_drift_metrics\n",
    "from metrics.metrics import compute_metrics\n",
    "\n",
    "# Configuration\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "ALPHA = 0.05  # significance level for hypothesis tests\n",
    "\n",
    "# Output directories\n",
    "RESULTS_DIR = os.path.join(PROJECT_ROOT, \"experiments\", \"results\")\n",
    "FIGURES_DIR = os.path.join(PROJECT_ROOT, \"experiments\", \"figures\")\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "os.makedirs(FIGURES_DIR, exist_ok=True)\n",
    "\n",
    "# Matplotlib configuration for publication-quality figures\n",
    "plt.rcParams.update({\n",
    "    'figure.figsize': (12, 6),\n",
    "    'figure.dpi': 150,\n",
    "    'font.size': 11,\n",
    "    'font.family': 'serif',\n",
    "    'axes.grid': True,\n",
    "    'grid.alpha': 0.3,\n",
    "    'savefig.dpi': 300,\n",
    "    'savefig.bbox': 'tight'\n",
    "})\n",
    "sns.set_palette(\"colorblind\")\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Results directory: {RESULTS_DIR}\")\n",
    "print(f\"Figures directory: {FIGURES_DIR}\")\n",
    "print(\"Setup complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec3058c",
   "metadata": {},
   "source": [
    "## Section 2: Research Questions and Hypotheses\n",
    "\n",
    "### Research Questions\n",
    "\n",
    "| ID | Research Question |\n",
    "|------|------|\n",
    "| **RQ1** | Can adversarial validation (binary classification between reference and current windows) detect unsupervised concept drift with competitive accuracy compared to existing unsupervised detectors? |\n",
    "| **RQ2** | Does SHAP-based feature attribution provide actionable explainability for detected drifts, correctly identifying which features shifted? |\n",
    "| **RQ3** | How does EADD perform across different drift types (abrupt, gradual, incremental, reoccurring)? |\n",
    "\n",
    "### Hypotheses\n",
    "\n",
    "| ID | Hypothesis | Test |\n",
    "|------|------|------|\n",
    "| **H1** | EADD achieves mean F1-score within 10% of the best-performing baseline detector across benchmark datasets. | Wilcoxon signed-rank test (paired) |\n",
    "| **H2** | EADD produces fewer false alarms than statistical-test-based detectors (KS, SPLL) on stationary segments. | One-sided Mann-Whitney U test |\n",
    "| **H3** | SHAP attributions correctly identify the drifting features in synthetic datasets where ground truth is known. | Spearman rank correlation |\n",
    "\n",
    "**Significance level:** α = 0.05 for all statistical tests (with Bonferroni correction for multiple comparisons)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121fedf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print research questions and hypotheses for documentation\n",
    "print(\"=\" * 70)\n",
    "print(\"  FORMAL RESEARCH QUESTIONS\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\"\"\n",
    "RQ1: Can adversarial validation detect unsupervised concept drift with\n",
    "     competitive accuracy compared to existing unsupervised detectors?\n",
    "\n",
    "RQ2: Does SHAP-based feature attribution provide actionable explainability\n",
    "     for detected drifts, correctly identifying which features shifted?\n",
    "\n",
    "RQ3: How does EADD perform across different drift types (abrupt, gradual,\n",
    "     incremental, reoccurring)?\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"  HYPOTHESES\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\"\"\n",
    "H1: EADD achieves mean F1-score within 10% of the best-performing\n",
    "    baseline detector across benchmark datasets.\n",
    "    Test: Wilcoxon signed-rank test (paired), α = {ALPHA}\n",
    "\n",
    "H2: EADD produces fewer false alarms than statistical-test-based\n",
    "    detectors on stationary segments.\n",
    "    Test: One-sided Mann-Whitney U test, α = {ALPHA}\n",
    "\n",
    "H3: SHAP attributions correctly identify the drifting features in\n",
    "    synthetic datasets where ground truth is known.\n",
    "    Test: Spearman rank correlation, α = {ALPHA}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86af8a5f",
   "metadata": {},
   "source": [
    "## Section 3: Load and Configure EADD Detector & Baselines\n",
    "\n",
    "Configure EADD with parameters from Appendix A of the thesis:\n",
    "- **LightGBM**: 100 estimators, lr=0.1, max_depth=-1, 31 leaves, AUC metric\n",
    "- **Windows**: Reference=500 (reservoir sampling), Current=200 (sliding), Frequency=50\n",
    "- **Permutation test**: B=100, α=0.01\n",
    "\n",
    "Configure all baseline detectors from the benchmark repository with default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56742a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════\n",
    "# EADD Configuration (from thesis Appendix A)\n",
    "# ═══════════════════════════════════════════════════════════\n",
    "\n",
    "EADD_CONFIG = {\n",
    "    'n_reference_samples': 500,\n",
    "    'n_current_samples': 200,\n",
    "    'auc_threshold': 0.7,\n",
    "    'n_permutations': 100,\n",
    "    'significance_level': 0.01,\n",
    "    'monitoring_frequency': 50,\n",
    "}\n",
    "\n",
    "def create_eadd(**kwargs):\n",
    "    \"\"\"Create a fresh EADD detector with thesis parameters.\"\"\"\n",
    "    config = {**EADD_CONFIG, **kwargs}\n",
    "    return ExplainableAdversarialDriftDetector(**config)\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════\n",
    "# Baseline Detectors Configuration\n",
    "# ═══════════════════════════════════════════════════════════\n",
    "\n",
    "def create_detector(name):\n",
    "    \"\"\"Create a fresh detector instance by name.\"\"\"\n",
    "    if name == \"EADD\":\n",
    "        return create_eadd()\n",
    "    elif name == \"D3\":\n",
    "        return D3(n_reference_samples=500, n_current_samples=200, \n",
    "                   auc_threshold=0.7, monitoring_frequency=50)\n",
    "    elif name == \"IBDD\" and IBDD is not None:\n",
    "        return IBDD()\n",
    "    elif name == \"OCDD\" and OCDD is not None:\n",
    "        return OCDD()\n",
    "    elif name == \"SPLL\" and SPLL is not None:\n",
    "        return SPLL()\n",
    "    elif name == \"BNDM\" and BNDM is not None:\n",
    "        return BNDM()\n",
    "    elif name == \"CSDDM\" and CSDDM is not None:\n",
    "        return CSDDM()\n",
    "    elif name == \"UDetect\" and UDetect is not None:\n",
    "        return UDetect()\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# List of all detectors to evaluate\n",
    "ALL_DETECTORS = [\"EADD\", \"D3\"]\n",
    "# Add available baseline detectors\n",
    "for det_name in [\"IBDD\", \"OCDD\", \"SPLL\", \"BNDM\", \"CSDDM\", \"UDetect\"]:\n",
    "    if create_detector(det_name) is not None:\n",
    "        ALL_DETECTORS.append(det_name)\n",
    "\n",
    "print(f\"Available detectors: {ALL_DETECTORS}\")\n",
    "print(f\"\\nEADD Configuration:\")\n",
    "for k, v in EADD_CONFIG.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a958aa3",
   "metadata": {},
   "source": [
    "## Section 4: Experiment 1 — Synthetic Dataset Evaluation (Temporal Drift Patterns)\n",
    "\n",
    "Evaluate EADD on synthetic datasets with **known drift injection points**, following Lukats et al. methodology. We generate four temporal drift patterns (abrupt, gradual, incremental, recurring) and compare detection accuracy against baselines.\n",
    "\n",
    "**Datasets:** SineClusters, WaveformDrift2, plus custom synthetic streams  \n",
    "**Metrics:** Precision, Recall, F1-score, Detection Delay, False Positive Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924c6447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════\n",
    "# Synthetic Data Generation for 4 Drift Patterns\n",
    "# ═══════════════════════════════════════════════════════════\n",
    "\n",
    "def generate_synthetic_stream(n_samples=10000, n_features=5, drift_point=5000, \n",
    "                               drift_type=\"abrupt\", seed=42):\n",
    "    \"\"\"Generate a synthetic data stream with a known drift point.\"\"\"\n",
    "    rng = np.random.RandomState(seed)\n",
    "    X = rng.randn(n_samples, n_features)\n",
    "    \n",
    "    if drift_type == \"abrupt\":\n",
    "        # Sudden shift in mean of features 0,1 at drift_point\n",
    "        X[drift_point:, 0] += 2.0\n",
    "        X[drift_point:, 1] += 1.5\n",
    "    elif drift_type == \"gradual\":\n",
    "        # Linear ramp over 1000 samples\n",
    "        transition = 1000\n",
    "        for i in range(drift_point, min(drift_point + transition, n_samples)):\n",
    "            progress = (i - drift_point) / transition\n",
    "            X[i, 0] += 2.0 * progress\n",
    "            X[i, 1] += 1.5 * progress\n",
    "        X[drift_point + transition:, 0] += 2.0\n",
    "        X[drift_point + transition:, 1] += 1.5\n",
    "    elif drift_type == \"incremental\":\n",
    "        # Slow continuous change\n",
    "        for i in range(drift_point, n_samples):\n",
    "            progress = min((i - drift_point) / 3000, 1.0)\n",
    "            X[i, 0] += 2.0 * progress\n",
    "            X[i, 1] += 1.0 * progress\n",
    "            X[i, 2] += 0.5 * progress\n",
    "    elif drift_type == \"recurring\":\n",
    "        # Alternating between two distributions\n",
    "        segment_len = 1500\n",
    "        for i in range(drift_point, n_samples):\n",
    "            segment = (i - drift_point) // segment_len\n",
    "            if segment % 2 == 0:\n",
    "                X[i, 0] += 2.0\n",
    "                X[i, 1] += 1.5\n",
    "    \n",
    "    return X\n",
    "\n",
    "DRIFT_TYPES = [\"abrupt\", \"gradual\", \"incremental\", \"recurring\"]\n",
    "N_RUNS = 5  # repeat each experiment for robustness\n",
    "\n",
    "def run_detector_on_stream(detector, X_stream):\n",
    "    \"\"\"Run a detector on a feature stream, return list of detection timestamps.\"\"\"\n",
    "    detections = []\n",
    "    for t in range(X_stream.shape[0]):\n",
    "        features = {f\"F{j}\": X_stream[t, j] for j in range(X_stream.shape[1])}\n",
    "        is_drift = detector.update(features)\n",
    "        if is_drift:\n",
    "            detections.append(t)\n",
    "    return detections\n",
    "\n",
    "def compute_detection_metrics(detections, true_drift, tolerance=500, n_total=10000):\n",
    "    \"\"\"Compute precision, recall, F1, delay from detections vs ground truth.\"\"\"\n",
    "    true_drifts = [true_drift] if isinstance(true_drift, (int, float)) else true_drift\n",
    "    \n",
    "    # True positives: detections within tolerance of a true drift\n",
    "    tp = 0\n",
    "    delays = []\n",
    "    matched_drifts = set()\n",
    "    for d in sorted(detections):\n",
    "        for td in true_drifts:\n",
    "            if td not in matched_drifts and abs(d - td) <= tolerance and d >= td:\n",
    "                tp += 1\n",
    "                delays.append(d - td)\n",
    "                matched_drifts.add(td)\n",
    "                break\n",
    "    \n",
    "    fp = len(detections) - tp\n",
    "    fn = len(true_drifts) - len(matched_drifts)\n",
    "    \n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    mean_delay = np.mean(delays) if delays else float('nan')\n",
    "    \n",
    "    return {\n",
    "        'precision': precision, 'recall': recall, 'f1': f1,\n",
    "        'mean_delay': mean_delay, 'false_positives': fp,\n",
    "        'true_positives': tp, 'false_negatives': fn\n",
    "    }\n",
    "\n",
    "print(\"Synthetic stream generator and metrics functions defined.\")\n",
    "print(f\"Drift types to evaluate: {DRIFT_TYPES}\")\n",
    "print(f\"Number of runs per experiment: {N_RUNS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da38fb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════\n",
    "# Run Experiment 1: Synthetic Temporal Drift Patterns\n",
    "# ═══════════════════════════════════════════════════════════\n",
    "\n",
    "exp1_results = []\n",
    "\n",
    "for drift_type in DRIFT_TYPES:\n",
    "    print(f\"\\n--- Drift Type: {drift_type.upper()} ---\")\n",
    "    \n",
    "    for det_name in [\"EADD\", \"D3\"]:\n",
    "        run_metrics = []\n",
    "        \n",
    "        for run in range(N_RUNS):\n",
    "            # Generate fresh synthetic stream\n",
    "            X = generate_synthetic_stream(\n",
    "                n_samples=10000, n_features=5, \n",
    "                drift_point=5000, drift_type=drift_type, seed=RANDOM_SEED + run\n",
    "            )\n",
    "            \n",
    "            # Create fresh detector\n",
    "            detector = create_detector(det_name)\n",
    "            if detector is None:\n",
    "                continue\n",
    "            \n",
    "            # Run detector\n",
    "            t_start = time.perf_counter()\n",
    "            detections = run_detector_on_stream(detector, X)\n",
    "            t_elapsed = time.perf_counter() - t_start\n",
    "            \n",
    "            # Compute metrics\n",
    "            metrics = compute_detection_metrics(detections, true_drift=5000)\n",
    "            metrics['runtime_sec'] = t_elapsed\n",
    "            metrics['n_detections'] = len(detections)\n",
    "            run_metrics.append(metrics)\n",
    "        \n",
    "        if run_metrics:\n",
    "            df_runs = pd.DataFrame(run_metrics)\n",
    "            exp1_results.append({\n",
    "                'drift_type': drift_type,\n",
    "                'detector': det_name,\n",
    "                'mean_f1': df_runs['f1'].mean(),\n",
    "                'std_f1': df_runs['f1'].std(),\n",
    "                'mean_precision': df_runs['precision'].mean(),\n",
    "                'mean_recall': df_runs['recall'].mean(),\n",
    "                'mean_delay': df_runs['mean_delay'].mean(),\n",
    "                'std_delay': df_runs['mean_delay'].std(),\n",
    "                'mean_false_positives': df_runs['false_positives'].mean(),\n",
    "                'mean_runtime': df_runs['runtime_sec'].mean(),\n",
    "                'mean_detections': df_runs['n_detections'].mean(),\n",
    "            })\n",
    "            print(f\"  {det_name}: F1={df_runs['f1'].mean():.3f}±{df_runs['f1'].std():.3f}, \"\n",
    "                  f\"Delay={df_runs['mean_delay'].mean():.0f}, FP={df_runs['false_positives'].mean():.1f}\")\n",
    "\n",
    "# Create results DataFrame\n",
    "df_exp1 = pd.DataFrame(exp1_results)\n",
    "df_exp1.to_csv(os.path.join(RESULTS_DIR, \"experiment1_synthetic.csv\"), index=False)\n",
    "print(f\"\\nExperiment 1 results saved. Shape: {df_exp1.shape}\")\n",
    "df_exp1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91bb559",
   "metadata": {},
   "source": [
    "## Section 5: Experiment 2 — Real-World Benchmark Evaluation\n",
    "\n",
    "Replicate Experiment 2 from Lukats et al. on real-world datasets. Load all available datasets from the benchmark repository, run EADD and baseline detectors, and collect results matching the `results/` directory format.\n",
    "\n",
    "**Datasets:** Electricity, Insects (5 variants), Luxembourg, NOAAWeather, OutdoorObjects, Ozone, PokerHand, Powersupply, RialtoBridgeTimelapse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f537be16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════\n",
    "# Load Real-World Datasets\n",
    "# ═══════════════════════════════════════════════════════════\n",
    "\n",
    "def load_real_world_datasets():\n",
    "    \"\"\"Load all available real-world datasets with metadata.\"\"\"\n",
    "    datasets_info = {}\n",
    "    loaders = {\n",
    "        'Elec2': electricity,\n",
    "        'InsectsAbruptBalanced': lambda: insects.load(variant='abrupt_balanced'),\n",
    "        'InsectsGradualBalanced': lambda: insects.load(variant='gradual_balanced'),\n",
    "        'InsectsIncrementalBalanced': lambda: insects.load(variant='incremental_balanced'),\n",
    "        'InsectsIncrementalAbruptBalanced': lambda: insects.load(variant='incremental_abrupt_balanced'),\n",
    "        'InsectsIncrementalReoccurringBalanced': lambda: insects.load(variant='incremental_reoccurring_balanced'),\n",
    "        'Luxembourg': luxembourg,\n",
    "        'NOAAWeather': noaa_weather,\n",
    "        'OutdoorObjects': outdoor_objects,\n",
    "        'Ozone': ozone,\n",
    "        'PokerHand': poker_hand,\n",
    "        'Powersupply': powersupply,\n",
    "        'RialtoBridgeTimelapse': rialto_bridge_timelapse,\n",
    "    }\n",
    "    \n",
    "    for name, loader in loaders.items():\n",
    "        try:\n",
    "            if callable(loader) and not hasattr(loader, 'load'):\n",
    "                dataset = loader()\n",
    "            else:\n",
    "                dataset = loader.load() if hasattr(loader, 'load') else loader()\n",
    "            \n",
    "            # Extract stream info\n",
    "            info = {\n",
    "                'name': name,\n",
    "                'dataset': dataset,\n",
    "                'drifts': getattr(dataset, 'drifts', []),\n",
    "            }\n",
    "            datasets_info[name] = info\n",
    "            n_drifts = len(info['drifts']) if info['drifts'] else 'unknown'\n",
    "            print(f\"  Loaded {name}: drifts={n_drifts}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  WARNING: Could not load {name}: {e}\")\n",
    "    \n",
    "    return datasets_info\n",
    "\n",
    "print(\"Loading real-world datasets...\")\n",
    "real_datasets = load_real_world_datasets()\n",
    "print(f\"\\nLoaded {len(real_datasets)} datasets.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e4097e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════\n",
    "# Run Experiment 2: Real-World Benchmark\n",
    "# ═══════════════════════════════════════════════════════════\n",
    "\n",
    "exp2_results = []\n",
    "\n",
    "for ds_name, ds_info in real_datasets.items():\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Dataset: {ds_name}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    dataset = ds_info['dataset']\n",
    "    true_drifts = ds_info.get('drifts', [])\n",
    "    \n",
    "    for det_name in [\"EADD\", \"D3\"]:\n",
    "        detector = create_detector(det_name)\n",
    "        if detector is None:\n",
    "            continue\n",
    "        \n",
    "        detections = []\n",
    "        n_samples = 0\n",
    "        t_start = time.perf_counter()\n",
    "        \n",
    "        try:\n",
    "            for i, (x, _) in enumerate(dataset):\n",
    "                # x is a dict of feature_name: value\n",
    "                if isinstance(x, dict):\n",
    "                    features = x\n",
    "                else:\n",
    "                    features = {f\"F{j}\": v for j, v in enumerate(x)}\n",
    "                \n",
    "                is_drift = detector.update(features)\n",
    "                if is_drift:\n",
    "                    detections.append(i)\n",
    "                n_samples += 1\n",
    "                \n",
    "                # Safety limit for very large datasets\n",
    "                if n_samples >= 100000:\n",
    "                    break\n",
    "        except Exception as e:\n",
    "            print(f\"  {det_name} error: {e}\")\n",
    "            continue\n",
    "        \n",
    "        t_elapsed = time.perf_counter() - t_start\n",
    "        \n",
    "        # Compute metrics if ground truth available\n",
    "        if true_drifts:\n",
    "            metrics = compute_detection_metrics(\n",
    "                detections, true_drifts, tolerance=500, n_total=n_samples\n",
    "            )\n",
    "        else:\n",
    "            metrics = {'precision': np.nan, 'recall': np.nan, 'f1': np.nan,\n",
    "                      'mean_delay': np.nan, 'false_positives': np.nan}\n",
    "        \n",
    "        result = {\n",
    "            'dataset': ds_name,\n",
    "            'detector': det_name,\n",
    "            'n_samples': n_samples,\n",
    "            'n_detections': len(detections),\n",
    "            'n_true_drifts': len(true_drifts) if true_drifts else 'unknown',\n",
    "            'runtime_sec': t_elapsed,\n",
    "            **metrics\n",
    "        }\n",
    "        exp2_results.append(result)\n",
    "        print(f\"  {det_name}: {len(detections)} detections in {t_elapsed:.1f}s, \"\n",
    "              f\"F1={metrics.get('f1', 'N/A')}\")\n",
    "\n",
    "df_exp2 = pd.DataFrame(exp2_results)\n",
    "df_exp2.to_csv(os.path.join(RESULTS_DIR, \"experiment2_realworld.csv\"), index=False)\n",
    "print(f\"\\nExperiment 2 results saved. Shape: {df_exp2.shape}\")\n",
    "df_exp2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997997bc",
   "metadata": {},
   "source": [
    "## Section 6: Experiment 3 — Explainability Analysis via SHAP\n",
    "\n",
    "The **core novel contribution** of EADD: SHAP-based feature attribution for detected drifts.\n",
    "\n",
    "For each detected drift:\n",
    "1. Extract the trained LightGBM adversarial classifier\n",
    "2. Compute SHAP values using TreeExplainer\n",
    "3. Classify drift type: univariate (>50%), subset (2-5 features >70%), or multivariate (no feature >30%)\n",
    "4. Validate against known ground truth for synthetic datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154c9ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════\n",
    "# Experiment 3: SHAP Explainability Case Study\n",
    "# ═══════════════════════════════════════════════════════════\n",
    "\n",
    "def run_explainability_experiment(n_samples=10000, n_features=8, seed=42):\n",
    "    \"\"\"\n",
    "    Generate 3 synthetic scenarios with known drifting features,\n",
    "    then verify SHAP correctly identifies them.\n",
    "    \"\"\"\n",
    "    rng = np.random.RandomState(seed)\n",
    "    results = []\n",
    "    \n",
    "    scenarios = {\n",
    "        \"univariate\": {\n",
    "            \"description\": \"Only feature F3 drifts\",\n",
    "            \"target_features\": [\"F3\"],\n",
    "            \"drift_fn\": lambda X, t: _apply_univariate(X, t)\n",
    "        },\n",
    "        \"subset\": {\n",
    "            \"description\": \"Features F2, F5, F7 drift together\",\n",
    "            \"target_features\": [\"F2\", \"F5\", \"F7\"],\n",
    "            \"drift_fn\": lambda X, t: _apply_subset(X, t)\n",
    "        },\n",
    "        \"multivariate\": {\n",
    "            \"description\": \"All features shift simultaneously\",\n",
    "            \"target_features\": [f\"F{i}\" for i in range(n_features)],\n",
    "            \"drift_fn\": lambda X, t: _apply_multivariate(X, t)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    for scenario_name, scenario in scenarios.items():\n",
    "        print(f\"\\n--- Scenario: {scenario_name} ---\")\n",
    "        print(f\"    {scenario['description']}\")\n",
    "        \n",
    "        # Generate data\n",
    "        X = rng.randn(n_samples, n_features)\n",
    "        drift_point = 5000\n",
    "        X = scenario[\"drift_fn\"](X.copy(), drift_point)\n",
    "        \n",
    "        # Run EADD\n",
    "        detector = create_eadd()\n",
    "        detections = []\n",
    "        for t in range(n_samples):\n",
    "            features = {f\"F{j}\": X[t, j] for j in range(n_features)}\n",
    "            is_drift = detector.update(features)\n",
    "            if is_drift:\n",
    "                detections.append(t)\n",
    "        \n",
    "        # Get SHAP report from last detection\n",
    "        report = detector.get_last_report() if hasattr(detector, 'get_last_report') else {}\n",
    "        \n",
    "        if report and 'feature_importance' in report:\n",
    "            importances = report['feature_importance']\n",
    "            total_imp = sum(importances.values())\n",
    "            \n",
    "            # Sort by importance\n",
    "            sorted_features = sorted(importances.items(), key=lambda x: -x[1])\n",
    "            top_feature = sorted_features[0][0]\n",
    "            top_pct = (sorted_features[0][1] / total_imp * 100) if total_imp > 0 else 0\n",
    "            \n",
    "            prescription = report.get('prescription_type', 'unknown')\n",
    "        else:\n",
    "            # Fallback: train adversarial classifier directly\n",
    "            from sklearn.model_selection import cross_val_score\n",
    "            X_ref = X[:drift_point][-500:]\n",
    "            X_cur = X[drift_point:][:500]\n",
    "            X_combined = np.vstack([X_ref, X_cur])\n",
    "            y_combined = np.array([0]*len(X_ref) + [1]*len(X_cur))\n",
    "            \n",
    "            model = lgb.LGBMClassifier(n_estimators=100, learning_rate=0.1, \n",
    "                                         max_depth=-1, num_leaves=31, verbose=-1)\n",
    "            model.fit(X_combined, y_combined)\n",
    "            auc = roc_auc_score(y_combined, model.predict_proba(X_combined)[:, 1])\n",
    "            \n",
    "            # SHAP analysis\n",
    "            explainer = shap.TreeExplainer(model)\n",
    "            shap_values = explainer.shap_values(X_combined)\n",
    "            if isinstance(shap_values, list):\n",
    "                shap_values = shap_values[1]\n",
    "            \n",
    "            feature_names = [f\"F{j}\" for j in range(n_features)]\n",
    "            mean_shap = np.abs(shap_values).mean(axis=0)\n",
    "            total_imp = mean_shap.sum()\n",
    "            \n",
    "            sorted_idx = np.argsort(-mean_shap)\n",
    "            top_feature = feature_names[sorted_idx[0]]\n",
    "            top_pct = (mean_shap[sorted_idx[0]] / total_imp * 100) if total_imp > 0 else 0\n",
    "            \n",
    "            importances = {feature_names[i]: mean_shap[i] for i in range(n_features)}\n",
    "            \n",
    "            # Determine prescription\n",
    "            if top_pct > 50:\n",
    "                prescription = \"univariate\"\n",
    "            elif sum(sorted(mean_shap, reverse=True)[:5]) / total_imp > 0.7:\n",
    "                prescription = \"subset\"\n",
    "            else:\n",
    "                prescription = \"multivariate\"\n",
    "        \n",
    "        # Validate against ground truth\n",
    "        correct = top_feature in scenario[\"target_features\"]\n",
    "        \n",
    "        result = {\n",
    "            'scenario': scenario_name,\n",
    "            'target': \", \".join(scenario[\"target_features\"]),\n",
    "            'n_detections': len(detections),\n",
    "            'top_feature': top_feature,\n",
    "            'top_importance_pct': round(top_pct, 1),\n",
    "            'prescription_type': prescription,\n",
    "            'correct_attribution': correct,\n",
    "            'importances': importances,\n",
    "        }\n",
    "        results.append(result)\n",
    "        \n",
    "        print(f\"    Detections: {len(detections)}\")\n",
    "        print(f\"    Top feature: {top_feature} ({top_pct:.1f}%)\")\n",
    "        print(f\"    Prescription: {prescription}\")\n",
    "        print(f\"    Correct: {'YES' if correct else 'NO'}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def _apply_univariate(X, t):\n",
    "    X[t:, 3] += 2.5\n",
    "    return X\n",
    "\n",
    "def _apply_subset(X, t):\n",
    "    X[t:, 2] += 1.5\n",
    "    X[t:, 5] += 2.0\n",
    "    X[t:, 7] += 1.8\n",
    "    return X\n",
    "\n",
    "def _apply_multivariate(X, t):\n",
    "    for j in range(X.shape[1]):\n",
    "        X[t:, j] += 0.5 + 0.3 * j / X.shape[1]\n",
    "    return X\n",
    "\n",
    "# Run experiment\n",
    "print(\"Running Experiment 3: SHAP Explainability...\")\n",
    "exp3_results = run_explainability_experiment()\n",
    "\n",
    "# Save results\n",
    "df_exp3 = pd.DataFrame([{k: v for k, v in r.items() if k != 'importances'} \n",
    "                          for r in exp3_results])\n",
    "df_exp3.to_csv(os.path.join(RESULTS_DIR, \"experiment3_explainability.csv\"), index=False)\n",
    "print(f\"\\nExperiment 3 results saved.\")\n",
    "df_exp3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4997f5",
   "metadata": {},
   "source": [
    "## Section 7: Experiment 4 — False Alarm Rate Analysis\n",
    "\n",
    "Evaluate false alarm rates on **stable data streams** (no drift). This directly tests **Hypothesis H2**: EADD produces fewer false alarms than statistical-test-based detectors due to the permutation test.\n",
    "\n",
    "Also analyze the effect of α (significance level) on false alarm rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3361e5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════\n",
    "# Experiment 4: False Alarm Robustness on Stable Data\n",
    "# ═══════════════════════════════════════════════════════════\n",
    "\n",
    "def generate_stable_stream(n_samples=10000, n_features=5, stream_type=\"gaussian\", seed=42):\n",
    "    \"\"\"Generate a stable (no-drift) data stream.\"\"\"\n",
    "    rng = np.random.RandomState(seed)\n",
    "    if stream_type == \"gaussian\":\n",
    "        return rng.randn(n_samples, n_features)\n",
    "    elif stream_type == \"uniform\":\n",
    "        return rng.uniform(-3, 3, size=(n_samples, n_features))\n",
    "    elif stream_type == \"noisy\":\n",
    "        X = rng.randn(n_samples, n_features)\n",
    "        X += rng.randn(n_samples, n_features) * 0.3  # extra noise\n",
    "        return X\n",
    "    elif stream_type == \"correlated\":\n",
    "        base = rng.randn(n_samples, 2)\n",
    "        X = np.zeros((n_samples, n_features))\n",
    "        for j in range(n_features):\n",
    "            X[:, j] = base[:, j % 2] + rng.randn(n_samples) * 0.5\n",
    "        return X\n",
    "\n",
    "STABLE_TYPES = [\"gaussian\", \"uniform\", \"noisy\", \"correlated\"]\n",
    "ALPHA_VALUES = [0.001, 0.005, 0.01, 0.05, 0.1]\n",
    "exp4_results = []\n",
    "\n",
    "for stream_type in STABLE_TYPES:\n",
    "    print(f\"\\n--- Stable Stream: {stream_type} ---\")\n",
    "    \n",
    "    # EADD with different alpha values\n",
    "    for alpha_val in [0.01]:  # thesis default\n",
    "        fa_counts = []\n",
    "        for run in range(N_RUNS):\n",
    "            X = generate_stable_stream(stream_type=stream_type, seed=RANDOM_SEED + run)\n",
    "            detector = create_eadd(significance_level=alpha_val)\n",
    "            detections = run_detector_on_stream(detector, X)\n",
    "            fa_counts.append(len(detections))\n",
    "        \n",
    "        exp4_results.append({\n",
    "            'stream_type': stream_type,\n",
    "            'detector': 'EADD',\n",
    "            'alpha': alpha_val,\n",
    "            'mean_false_alarms': np.mean(fa_counts),\n",
    "            'std_false_alarms': np.std(fa_counts),\n",
    "            'max_false_alarms': np.max(fa_counts),\n",
    "        })\n",
    "        print(f\"  EADD (α={alpha_val}): {np.mean(fa_counts):.1f}±{np.std(fa_counts):.1f} FA\")\n",
    "    \n",
    "    # D3 with different thresholds\n",
    "    for threshold in [0.6, 0.7, 0.8]:\n",
    "        fa_counts = []\n",
    "        for run in range(N_RUNS):\n",
    "            X = generate_stable_stream(stream_type=stream_type, seed=RANDOM_SEED + run)\n",
    "            detector = D3(n_reference_samples=500, n_current_samples=200,\n",
    "                          auc_threshold=threshold, monitoring_frequency=50)\n",
    "            detections = run_detector_on_stream(detector, X)\n",
    "            fa_counts.append(len(detections))\n",
    "        \n",
    "        exp4_results.append({\n",
    "            'stream_type': stream_type,\n",
    "            'detector': f'D3(τ={threshold})',\n",
    "            'alpha': threshold,\n",
    "            'mean_false_alarms': np.mean(fa_counts),\n",
    "            'std_false_alarms': np.std(fa_counts),\n",
    "            'max_false_alarms': np.max(fa_counts),\n",
    "        })\n",
    "        print(f\"  D3 (τ={threshold}): {np.mean(fa_counts):.1f}±{np.std(fa_counts):.1f} FA\")\n",
    "\n",
    "# Alpha sensitivity analysis for EADD\n",
    "print(\"\\n--- EADD Alpha Sensitivity ---\")\n",
    "alpha_sensitivity = []\n",
    "for alpha_val in ALPHA_VALUES:\n",
    "    fa_all = []\n",
    "    for stream_type in STABLE_TYPES:\n",
    "        for run in range(N_RUNS):\n",
    "            X = generate_stable_stream(stream_type=stream_type, seed=RANDOM_SEED + run)\n",
    "            detector = create_eadd(significance_level=alpha_val)\n",
    "            detections = run_detector_on_stream(detector, X)\n",
    "            fa_all.append(len(detections))\n",
    "    alpha_sensitivity.append({\n",
    "        'alpha': alpha_val,\n",
    "        'mean_fa': np.mean(fa_all),\n",
    "        'std_fa': np.std(fa_all),\n",
    "    })\n",
    "    print(f\"  α={alpha_val}: {np.mean(fa_all):.1f} ± {np.std(fa_all):.1f} mean FA\")\n",
    "\n",
    "df_exp4 = pd.DataFrame(exp4_results)\n",
    "df_exp4.to_csv(os.path.join(RESULTS_DIR, \"experiment4_false_alarms.csv\"), index=False)\n",
    "df_alpha = pd.DataFrame(alpha_sensitivity)\n",
    "df_alpha.to_csv(os.path.join(RESULTS_DIR, \"experiment4_alpha_sensitivity.csv\"), index=False)\n",
    "print(f\"\\nExperiment 4 results saved.\")\n",
    "df_exp4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3966489d",
   "metadata": {},
   "source": [
    "## Section 8: Experiment 5 — Detection Latency Analysis\n",
    "\n",
    "Measure detection latency (delay from true drift to first detection) and wall-clock runtime for each detector. Analyze the latency-vs-accuracy trade-off, comparing EADD's computational overhead (LightGBM + SHAP) against lighter detectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9475c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════\n",
    "# Experiment 5: Detection Latency Analysis\n",
    "# ═══════════════════════════════════════════════════════════\n",
    "\n",
    "exp5_results = []\n",
    "\n",
    "# Use synthetic streams for controlled latency measurement\n",
    "for drift_type in DRIFT_TYPES:\n",
    "    for det_name in [\"EADD\", \"D3\"]:\n",
    "        delays_all = []\n",
    "        runtimes_all = []\n",
    "        \n",
    "        for run in range(N_RUNS):\n",
    "            X = generate_synthetic_stream(\n",
    "                n_samples=10000, n_features=5,\n",
    "                drift_point=5000, drift_type=drift_type, seed=RANDOM_SEED + run\n",
    "            )\n",
    "            detector = create_detector(det_name)\n",
    "            if detector is None:\n",
    "                continue\n",
    "            \n",
    "            t_start = time.perf_counter()\n",
    "            detections = run_detector_on_stream(detector, X)\n",
    "            t_elapsed = time.perf_counter() - t_start\n",
    "            runtimes_all.append(t_elapsed)\n",
    "            \n",
    "            # Find first detection after drift point\n",
    "            post_drift = [d for d in detections if d >= 5000]\n",
    "            if post_drift:\n",
    "                delays_all.append(post_drift[0] - 5000)\n",
    "        \n",
    "        if delays_all:\n",
    "            exp5_results.append({\n",
    "                'drift_type': drift_type,\n",
    "                'detector': det_name,\n",
    "                'mean_delay': np.mean(delays_all),\n",
    "                'median_delay': np.median(delays_all),\n",
    "                'std_delay': np.std(delays_all),\n",
    "                'mean_runtime_sec': np.mean(runtimes_all),\n",
    "                'std_runtime_sec': np.std(runtimes_all),\n",
    "                'detection_rate': len(delays_all) / N_RUNS,\n",
    "            })\n",
    "\n",
    "df_exp5 = pd.DataFrame(exp5_results)\n",
    "df_exp5.to_csv(os.path.join(RESULTS_DIR, \"experiment5_latency.csv\"), index=False)\n",
    "print(\"Experiment 5: Detection Latency Results\")\n",
    "print(\"=\"*60)\n",
    "df_exp5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e198db1",
   "metadata": {},
   "source": [
    "## Section 9: Experiment 6 — Comparison with Baseline Detectors (Existing Results)\n",
    "\n",
    "Load existing baseline results from the `results/` directory CSVs and merge with EADD results for comprehensive comparison. Compute overall rankings using the same methodology as Lukats et al."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3697734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════\n",
    "# Load Existing Baseline Results from results/ Directory\n",
    "# ═══════════════════════════════════════════════════════════\n",
    "\n",
    "RESULTS_BASE = os.path.join(PROJECT_ROOT, \"results\")\n",
    "baseline_detectors = [\"bndm\", \"csddm\", \"d3\", \"ibdd\", \"ocdd\", \"spll\", \"udetect\"]\n",
    "\n",
    "# Load all available baseline results\n",
    "all_baseline_results = {}\n",
    "for ds_dir in os.listdir(RESULTS_BASE):\n",
    "    ds_path = os.path.join(RESULTS_BASE, ds_dir)\n",
    "    if not os.path.isdir(ds_path):\n",
    "        continue\n",
    "    \n",
    "    for det_file in os.listdir(ds_path):\n",
    "        if det_file.endswith('.csv'):\n",
    "            det_name = det_file.replace('.csv', '')\n",
    "            try:\n",
    "                df = pd.read_csv(os.path.join(ds_path, det_file))\n",
    "                key = (ds_dir, det_name)\n",
    "                all_baseline_results[key] = df\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "print(f\"Loaded {len(all_baseline_results)} baseline result files.\")\n",
    "print(f\"\\nDatasets with results:\")\n",
    "datasets_with_results = sorted(set(k[0] for k in all_baseline_results.keys()))\n",
    "for ds in datasets_with_results:\n",
    "    dets = sorted([k[1] for k in all_baseline_results.keys() if k[0] == ds])\n",
    "    print(f\"  {ds}: {', '.join(dets)}\")\n",
    "\n",
    "# Parse detection counts from baseline results\n",
    "baseline_summary = []\n",
    "for (ds, det), df in all_baseline_results.items():\n",
    "    # Count rows as detection count (each row is a detected drift)\n",
    "    n_detections = len(df)\n",
    "    baseline_summary.append({\n",
    "        'dataset': ds,\n",
    "        'detector': det,\n",
    "        'n_detections': n_detections,\n",
    "    })\n",
    "\n",
    "df_baseline = pd.DataFrame(baseline_summary)\n",
    "print(f\"\\nBaseline summary shape: {df_baseline.shape}\")\n",
    "\n",
    "# Pivot table: detections per dataset per detector\n",
    "pivot_detections = df_baseline.pivot_table(\n",
    "    index='dataset', columns='detector', values='n_detections', aggfunc='first'\n",
    ")\n",
    "print(\"\\nDetection counts by dataset and detector:\")\n",
    "pivot_detections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4857e6",
   "metadata": {},
   "source": [
    "## Section 10: Statistical Hypothesis Testing\n",
    "\n",
    "Perform formal statistical tests for all three hypotheses:\n",
    "\n",
    "- **H1:** Wilcoxon signed-rank test (EADD F1 vs baselines), Friedman test + Nemenyi post-hoc\n",
    "- **H2:** One-sided Mann-Whitney U test (EADD false alarms vs D3)\n",
    "- **H3:** Spearman rank correlation (SHAP importance vs ground truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab049e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════\n",
    "# Statistical Hypothesis Testing\n",
    "# ═══════════════════════════════════════════════════════════\n",
    "\n",
    "hypothesis_results = []\n",
    "\n",
    "# ─── H1: Detection Effectiveness ─────────────────────────\n",
    "print(\"=\" * 60)\n",
    "print(\"H1: EADD Detection Effectiveness vs Baselines\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Use Experiment 1 F1 scores (paired by drift type)\n",
    "eadd_f1 = df_exp1[df_exp1['detector'] == 'EADD']['mean_f1'].values\n",
    "d3_f1 = df_exp1[df_exp1['detector'] == 'D3']['mean_f1'].values\n",
    "\n",
    "if len(eadd_f1) >= 3 and len(d3_f1) >= 3:\n",
    "    # Wilcoxon signed-rank test (paired)\n",
    "    stat, p_val = stats.wilcoxon(eadd_f1, d3_f1)\n",
    "    effect_size = np.mean(eadd_f1 - d3_f1) / np.std(eadd_f1 - d3_f1) if np.std(eadd_f1 - d3_f1) > 0 else 0\n",
    "    \n",
    "    print(f\"  Wilcoxon signed-rank test:\")\n",
    "    print(f\"    W-statistic: {stat:.4f}\")\n",
    "    print(f\"    p-value: {p_val:.6f}\")\n",
    "    print(f\"    Effect size (Cohen's d): {effect_size:.4f}\")\n",
    "    print(f\"    Decision: {'REJECT H0' if p_val < ALPHA else 'FAIL TO REJECT H0'} at α={ALPHA}\")\n",
    "    print(f\"    EADD mean F1: {np.mean(eadd_f1):.4f}, D3 mean F1: {np.mean(d3_f1):.4f}\")\n",
    "    \n",
    "    hypothesis_results.append({\n",
    "        'hypothesis': 'H1', 'test': 'Wilcoxon signed-rank',\n",
    "        'comparison': 'EADD vs D3 (F1)', 'statistic': stat,\n",
    "        'p_value': p_val, 'effect_size': effect_size,\n",
    "        'significant': p_val < ALPHA, 'alpha': ALPHA\n",
    "    })\n",
    "else:\n",
    "    print(\"  Insufficient paired data for Wilcoxon test.\")\n",
    "\n",
    "# ─── H2: False Alarm Reduction ──────────────────────────\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"H2: EADD Produces Fewer False Alarms than D3\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "eadd_fa = df_exp4[df_exp4['detector'] == 'EADD']['mean_false_alarms'].values\n",
    "d3_fa = df_exp4[df_exp4['detector'] == 'D3(τ=0.7)']['mean_false_alarms'].values\n",
    "\n",
    "if len(eadd_fa) > 0 and len(d3_fa) > 0:\n",
    "    stat, p_val = stats.mannwhitneyu(eadd_fa, d3_fa, alternative='less')\n",
    "    \n",
    "    print(f\"  Mann-Whitney U test (one-sided, EADD < D3):\")\n",
    "    print(f\"    U-statistic: {stat:.4f}\")\n",
    "    print(f\"    p-value: {p_val:.6f}\")\n",
    "    print(f\"    Decision: {'REJECT H0' if p_val < ALPHA else 'FAIL TO REJECT H0'} at α={ALPHA}\")\n",
    "    print(f\"    EADD mean FA: {np.mean(eadd_fa):.2f}, D3 mean FA: {np.mean(d3_fa):.2f}\")\n",
    "    \n",
    "    hypothesis_results.append({\n",
    "        'hypothesis': 'H2', 'test': 'Mann-Whitney U (one-sided)',\n",
    "        'comparison': 'EADD FA vs D3 FA', 'statistic': stat,\n",
    "        'p_value': p_val, 'effect_size': np.nan,\n",
    "        'significant': p_val < ALPHA, 'alpha': ALPHA\n",
    "    })\n",
    "\n",
    "# ─── H3: SHAP Attribution Accuracy ──────────────────────\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"H3: SHAP Correctly Identifies Drifting Features\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# For each scenario, check if top feature matches ground truth\n",
    "n_correct = sum(r['correct_attribution'] for r in exp3_results)\n",
    "n_total = len(exp3_results)\n",
    "accuracy = n_correct / n_total if n_total > 0 else 0\n",
    "\n",
    "print(f\"  Attribution accuracy: {n_correct}/{n_total} = {accuracy:.1%}\")\n",
    "print(f\"  Scenarios:\")\n",
    "for r in exp3_results:\n",
    "    print(f\"    {r['scenario']}: top={r['top_feature']} ({r['top_importance_pct']:.1f}%), \"\n",
    "          f\"prescription={r['prescription_type']}, correct={r['correct_attribution']}\")\n",
    "\n",
    "# One-sample binomial test: is accuracy significantly above chance?\n",
    "# Chance level = 1/n_features\n",
    "n_features_test = 8\n",
    "p_chance = 1.0 / n_features_test\n",
    "binom_p = stats.binom_test(n_correct, n_total, p_chance, alternative='greater')\n",
    "print(f\"\\n  Binomial test (accuracy > chance={p_chance:.3f}):\")\n",
    "print(f\"    p-value: {binom_p:.6f}\")\n",
    "print(f\"    Decision: {'REJECT H0' if binom_p < ALPHA else 'FAIL TO REJECT H0'}\")\n",
    "\n",
    "hypothesis_results.append({\n",
    "    'hypothesis': 'H3', 'test': 'Binomial test',\n",
    "    'comparison': f'SHAP accuracy vs chance ({p_chance:.3f})',\n",
    "    'statistic': n_correct, 'p_value': binom_p,\n",
    "    'effect_size': accuracy, 'significant': binom_p < ALPHA, 'alpha': ALPHA\n",
    "})\n",
    "\n",
    "# Save hypothesis test results\n",
    "df_hyp = pd.DataFrame(hypothesis_results)\n",
    "df_hyp.to_csv(os.path.join(RESULTS_DIR, \"hypothesis_tests.csv\"), index=False)\n",
    "print(f\"\\n\\nHypothesis test results saved.\")\n",
    "df_hyp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a65dd7c",
   "metadata": {},
   "source": [
    "## Section 11: Results Visualization — Detection Performance Heatmaps\n",
    "\n",
    "Publication-quality heatmaps showing F1-score, precision, and recall across all detectors and datasets. Follows the layout from Lukats et al. Tables 4-5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7268d13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════\n",
    "# Visualization: Detection Performance Heatmaps\n",
    "# ═══════════════════════════════════════════════════════════\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "fig.suptitle('Experiment 1: Detection Performance on Synthetic Datasets', \n",
    "             fontsize=14, fontweight='bold')\n",
    "\n",
    "# Reshape data for heatmaps\n",
    "for idx, (metric, title) in enumerate([\n",
    "    ('mean_f1', 'F1-Score'), \n",
    "    ('mean_precision', 'Precision'), \n",
    "    ('mean_recall', 'Recall')\n",
    "]):\n",
    "    pivot = df_exp1.pivot_table(index='drift_type', columns='detector', values=metric)\n",
    "    \n",
    "    sns.heatmap(pivot, annot=True, fmt='.3f', cmap='YlGnBu', ax=axes[idx],\n",
    "                vmin=0, vmax=1, linewidths=0.5, cbar_kws={'shrink': 0.8})\n",
    "    axes[idx].set_title(title, fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_xlabel('Detector')\n",
    "    axes[idx].set_ylabel('Drift Type' if idx == 0 else '')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(FIGURES_DIR, \"exp1_performance_heatmap.png\"), dpi=300)\n",
    "plt.savefig(os.path.join(FIGURES_DIR, \"exp1_performance_heatmap.pdf\"))\n",
    "plt.show()\n",
    "print(\"Heatmap saved to experiments/figures/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9804d440",
   "metadata": {},
   "source": [
    "## Section 12: Results Visualization — ROC and Precision-Recall Curves\n",
    "\n",
    "Plot the ROC curve of EADD's adversarial classifier at true drift points vs non-drift points. Also show AUC time series across complete data streams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddd4a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════\n",
    "# ROC Curves: Adversarial Classifier at Drift vs Non-Drift\n",
    "# ═══════════════════════════════════════════════════════════\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "fig.suptitle('Adversarial Classifier ROC: Drift vs Non-Drift Points', \n",
    "             fontsize=14, fontweight='bold')\n",
    "\n",
    "# Scenario 1: At a drift point (abrupt)\n",
    "X_drift = generate_synthetic_stream(drift_point=5000, drift_type=\"abrupt\", seed=42)\n",
    "X_ref = X_drift[4500:5000]\n",
    "X_cur = X_drift[5000:5500]\n",
    "X_roc = np.vstack([X_ref, X_cur])\n",
    "y_roc = np.array([0]*len(X_ref) + [1]*len(X_cur))\n",
    "\n",
    "model_drift = lgb.LGBMClassifier(n_estimators=100, learning_rate=0.1, verbose=-1)\n",
    "model_drift.fit(X_roc, y_roc)\n",
    "y_prob_drift = model_drift.predict_proba(X_roc)[:, 1]\n",
    "fpr_d, tpr_d, _ = roc_curve(y_roc, y_prob_drift)\n",
    "auc_drift = roc_auc_score(y_roc, y_prob_drift)\n",
    "\n",
    "axes[0].plot(fpr_d, tpr_d, 'b-', linewidth=2, label=f'At Drift (AUC={auc_drift:.3f})')\n",
    "axes[0].plot([0, 1], [0, 1], 'k--', alpha=0.5, label='Random (AUC=0.500)')\n",
    "axes[0].set_title('(a) At Drift Point (t=5000)', fontsize=12)\n",
    "axes[0].set_xlabel('False Positive Rate')\n",
    "axes[0].set_ylabel('True Positive Rate')\n",
    "axes[0].legend(loc='lower right')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Scenario 2: In stable region (no drift)\n",
    "X_ref2 = X_drift[2000:2500]\n",
    "X_cur2 = X_drift[2500:3000]\n",
    "X_roc2 = np.vstack([X_ref2, X_cur2])\n",
    "y_roc2 = np.array([0]*len(X_ref2) + [1]*len(X_cur2))\n",
    "\n",
    "model_stable = lgb.LGBMClassifier(n_estimators=100, learning_rate=0.1, verbose=-1)\n",
    "model_stable.fit(X_roc2, y_roc2)\n",
    "y_prob_stable = model_stable.predict_proba(X_roc2)[:, 1]\n",
    "fpr_s, tpr_s, _ = roc_curve(y_roc2, y_prob_stable)\n",
    "auc_stable = roc_auc_score(y_roc2, y_prob_stable)\n",
    "\n",
    "axes[1].plot(fpr_s, tpr_s, 'r-', linewidth=2, label=f'No Drift (AUC={auc_stable:.3f})')\n",
    "axes[1].plot([0, 1], [0, 1], 'k--', alpha=0.5, label='Random (AUC=0.500)')\n",
    "axes[1].set_title('(b) Stable Region (t=2500)', fontsize=12)\n",
    "axes[1].set_xlabel('False Positive Rate')\n",
    "axes[1].set_ylabel('True Positive Rate')\n",
    "axes[1].legend(loc='lower right')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(FIGURES_DIR, \"roc_drift_vs_stable.png\"), dpi=300)\n",
    "plt.savefig(os.path.join(FIGURES_DIR, \"roc_drift_vs_stable.pdf\"))\n",
    "plt.show()\n",
    "print(f\"Drift AUC: {auc_drift:.3f}, Stable AUC: {auc_stable:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7b1887",
   "metadata": {},
   "source": [
    "## Section 13: Results Visualization — SHAP Feature Attribution Plots\n",
    "\n",
    "Generate SHAP visualizations: bar charts, beeswarm plots, and drift-type distribution across datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b084fcf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════\n",
    "# SHAP Feature Attribution Visualizations\n",
    "# ═══════════════════════════════════════════════════════════\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "fig.suptitle('Experiment 3: SHAP Feature Attribution Accuracy', \n",
    "             fontsize=14, fontweight='bold')\n",
    "\n",
    "for idx, r in enumerate(exp3_results):\n",
    "    ax = axes[idx]\n",
    "    importances = r.get('importances', {})\n",
    "    if importances:\n",
    "        features = list(importances.keys())\n",
    "        values = [importances[f] for f in features]\n",
    "        total = sum(values)\n",
    "        pcts = [v / total * 100 for v in values]\n",
    "        \n",
    "        # Color ground truth features differently\n",
    "        target_features = r['target'].split(\", \")\n",
    "        colors = ['#2196F3' if f in target_features else '#90CAF9' for f in features]\n",
    "        \n",
    "        bars = ax.barh(features, pcts, color=colors)\n",
    "        ax.set_xlabel('Importance (%)')\n",
    "        ax.set_title(f\"({chr(97+idx)}) {r['scenario'].title()}\\nTarget: {r['target']}\")\n",
    "        ax.axvline(x=50, color='red', linestyle='--', alpha=0.5, label='50% threshold')\n",
    "        \n",
    "        # Add value labels\n",
    "        for bar, pct in zip(bars, pcts):\n",
    "            ax.text(bar.get_width() + 0.5, bar.get_y() + bar.get_height()/2,\n",
    "                    f'{pct:.1f}%', va='center', fontsize=9)\n",
    "    \n",
    "    ax.legend(['Ground truth', '50% threshold'], loc='lower right', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(FIGURES_DIR, \"shap_attribution_accuracy.png\"), dpi=300)\n",
    "plt.savefig(os.path.join(FIGURES_DIR, \"shap_attribution_accuracy.pdf\"))\n",
    "plt.show()\n",
    "print(\"SHAP attribution plots saved.\")\n",
    "\n",
    "# SHAP Beeswarm Plot (for the univariate scenario)\n",
    "print(\"\\nGenerating SHAP beeswarm plot for univariate scenario...\")\n",
    "X_demo = generate_synthetic_stream(drift_point=5000, drift_type=\"abrupt\", seed=42)\n",
    "X_ref_demo = X_demo[4500:5000]\n",
    "X_cur_demo = X_demo[5000:5500]\n",
    "X_combined = np.vstack([X_ref_demo, X_cur_demo])\n",
    "y_combined = np.array([0]*len(X_ref_demo) + [1]*len(X_cur_demo))\n",
    "\n",
    "model_demo = lgb.LGBMClassifier(n_estimators=100, learning_rate=0.1, verbose=-1)\n",
    "model_demo.fit(X_combined, y_combined)\n",
    "\n",
    "explainer = shap.TreeExplainer(model_demo)\n",
    "shap_values = explainer.shap_values(X_combined)\n",
    "if isinstance(shap_values, list):\n",
    "    shap_values = shap_values[1]\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "shap.summary_plot(shap_values, X_combined, \n",
    "                  feature_names=[f\"F{i}\" for i in range(5)],\n",
    "                  show=False)\n",
    "plt.title(\"SHAP Feature Importance: Adversarial Classifier at Drift Point\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(FIGURES_DIR, \"shap_beeswarm.png\"), dpi=300)\n",
    "plt.savefig(os.path.join(FIGURES_DIR, \"shap_beeswarm.pdf\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4627a333",
   "metadata": {},
   "source": [
    "## Section 14: Results Visualization — Latency and Runtime Boxplots\n",
    "\n",
    "Compare detection latency and wall-clock runtime across detectors. Analyze the latency-vs-accuracy trade-off (Pareto front)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01de5fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════\n",
    "# Latency and Runtime Visualizations\n",
    "# ═══════════════════════════════════════════════════════════\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('Detection Latency and Runtime Analysis', fontsize=14, fontweight='bold')\n",
    "\n",
    "# (a) Detection Delay by Drift Type\n",
    "ax = axes[0, 0]\n",
    "if not df_exp5.empty:\n",
    "    x = np.arange(len(DRIFT_TYPES))\n",
    "    width = 0.35\n",
    "    for i, det in enumerate([\"EADD\", \"D3\"]):\n",
    "        data = df_exp5[df_exp5['detector'] == det]\n",
    "        if not data.empty:\n",
    "            delays = [data[data['drift_type'] == dt]['mean_delay'].values[0] \n",
    "                      if len(data[data['drift_type'] == dt]) > 0 else 0 \n",
    "                      for dt in DRIFT_TYPES]\n",
    "            ax.bar(x + i*width - width/2, delays, width, label=det, \n",
    "                   color=['#2196F3', '#FF9800'][i])\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels([dt.title() for dt in DRIFT_TYPES])\n",
    "    ax.set_ylabel('Mean Detection Delay (samples)')\n",
    "    ax.set_title('(a) Detection Delay by Drift Type')\n",
    "    ax.legend()\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# (b) Runtime Comparison\n",
    "ax = axes[0, 1]\n",
    "if not df_exp5.empty:\n",
    "    for i, det in enumerate([\"EADD\", \"D3\"]):\n",
    "        data = df_exp5[df_exp5['detector'] == det]\n",
    "        if not data.empty:\n",
    "            runtimes = data['mean_runtime_sec'].values\n",
    "            ax.bar(x + i*width - width/2, \n",
    "                   [data[data['drift_type'] == dt]['mean_runtime_sec'].values[0]\n",
    "                    if len(data[data['drift_type'] == dt]) > 0 else 0\n",
    "                    for dt in DRIFT_TYPES],\n",
    "                   width, label=det, color=['#2196F3', '#FF9800'][i])\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels([dt.title() for dt in DRIFT_TYPES])\n",
    "    ax.set_ylabel('Runtime (seconds)')\n",
    "    ax.set_title('(b) Wall-Clock Runtime')\n",
    "    ax.legend()\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# (c) F1 vs Delay Trade-off (Pareto front)\n",
    "ax = axes[1, 0]\n",
    "if not df_exp1.empty:\n",
    "    for det in [\"EADD\", \"D3\"]:\n",
    "        data = df_exp1[df_exp1['detector'] == det]\n",
    "        ax.scatter(data['mean_delay'], data['mean_f1'], s=100, label=det,\n",
    "                   marker='o' if det == 'EADD' else 's', zorder=5)\n",
    "        for _, row in data.iterrows():\n",
    "            ax.annotate(row['drift_type'], (row['mean_delay'], row['mean_f1']),\n",
    "                        fontsize=8, ha='left', va='bottom')\n",
    "ax.set_xlabel('Mean Detection Delay (samples)')\n",
    "ax.set_ylabel('Mean F1-Score')\n",
    "ax.set_title('(c) F1 vs Detection Delay Trade-off')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# (d) False Alarms on Stable Data\n",
    "ax = axes[1, 1]\n",
    "if not df_exp4.empty:\n",
    "    eadd_data = df_exp4[df_exp4['detector'] == 'EADD']\n",
    "    d3_data = df_exp4[df_exp4['detector'] == 'D3(τ=0.7)']\n",
    "    x = np.arange(len(STABLE_TYPES))\n",
    "    if not eadd_data.empty and not d3_data.empty:\n",
    "        ax.bar(x - width/2, eadd_data['mean_false_alarms'].values, width, \n",
    "               label='EADD', color='#2196F3')\n",
    "        ax.bar(x + width/2, d3_data['mean_false_alarms'].values, width, \n",
    "               label='D3 (τ=0.7)', color='#FF9800')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels([st.title() for st in STABLE_TYPES])\n",
    "    ax.set_ylabel('False Alarms')\n",
    "    ax.set_title('(d) False Alarms on Stable Data')\n",
    "    ax.legend()\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(FIGURES_DIR, \"latency_runtime_analysis.png\"), dpi=300)\n",
    "plt.savefig(os.path.join(FIGURES_DIR, \"latency_runtime_analysis.pdf\"))\n",
    "plt.show()\n",
    "print(\"Latency and runtime plots saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b24101",
   "metadata": {},
   "source": [
    "## Section 15: Discussion — Strengths, Limitations, and Drift Type Analysis\n",
    "\n",
    "Compute performance breakdown by drift type, identify strengths/weaknesses, and analyze correlations between dataset characteristics and EADD's relative performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fb9f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════\n",
    "# Discussion: Strengths, Limitations, Drift Type Analysis\n",
    "# ═══════════════════════════════════════════════════════════\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"  DISCUSSION: KEY FINDINGS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Performance by drift type\n",
    "print(\"\\n1. PERFORMANCE BY DRIFT TYPE\")\n",
    "print(\"-\" * 40)\n",
    "if not df_exp1.empty:\n",
    "    eadd_by_type = df_exp1[df_exp1['detector'] == 'EADD'][\n",
    "        ['drift_type', 'mean_f1', 'mean_precision', 'mean_recall', 'mean_delay']\n",
    "    ]\n",
    "    print(eadd_by_type.to_string(index=False))\n",
    "    \n",
    "    best_type = eadd_by_type.loc[eadd_by_type['mean_f1'].idxmax(), 'drift_type']\n",
    "    worst_type = eadd_by_type.loc[eadd_by_type['mean_f1'].idxmin(), 'drift_type']\n",
    "    print(f\"\\n  Best drift type: {best_type}\")\n",
    "    print(f\"  Worst drift type: {worst_type}\")\n",
    "\n",
    "# Comparison summary\n",
    "print(\"\\n2. EADD vs D3 COMPARISON\")\n",
    "print(\"-\" * 40)\n",
    "if not df_exp1.empty:\n",
    "    eadd_f1_avg = df_exp1[df_exp1['detector'] == 'EADD']['mean_f1'].mean()\n",
    "    d3_f1_avg = df_exp1[df_exp1['detector'] == 'D3']['mean_f1'].mean()\n",
    "    improvement = (eadd_f1_avg - d3_f1_avg) / d3_f1_avg * 100 if d3_f1_avg > 0 else 0\n",
    "    print(f\"  EADD avg F1: {eadd_f1_avg:.4f}\")\n",
    "    print(f\"  D3 avg F1: {d3_f1_avg:.4f}\")\n",
    "    print(f\"  Improvement: {improvement:+.1f}%\")\n",
    "\n",
    "# Explainability summary\n",
    "print(\"\\n3. EXPLAINABILITY ACCURACY\")\n",
    "print(\"-\" * 40)\n",
    "n_correct = sum(r['correct_attribution'] for r in exp3_results)\n",
    "print(f\"  SHAP attribution accuracy: {n_correct}/{len(exp3_results)} scenarios correct\")\n",
    "for r in exp3_results:\n",
    "    status = \"CORRECT\" if r['correct_attribution'] else \"INCORRECT\"\n",
    "    print(f\"    {r['scenario']}: {status} (top={r['top_feature']}, {r['top_importance_pct']:.1f}%)\")\n",
    "\n",
    "# False alarm summary\n",
    "print(\"\\n4. FALSE ALARM ROBUSTNESS\")\n",
    "print(\"-\" * 40)\n",
    "if not df_exp4.empty:\n",
    "    eadd_fa_avg = df_exp4[df_exp4['detector'] == 'EADD']['mean_false_alarms'].mean()\n",
    "    d3_fa_avg = df_exp4[df_exp4['detector'] == 'D3(τ=0.7)']['mean_false_alarms'].mean()\n",
    "    print(f\"  EADD avg false alarms: {eadd_fa_avg:.2f}\")\n",
    "    print(f\"  D3 (τ=0.7) avg false alarms: {d3_fa_avg:.2f}\")\n",
    "    reduction = (1 - eadd_fa_avg / d3_fa_avg) * 100 if d3_fa_avg > 0 else 0\n",
    "    print(f\"  False alarm reduction: {reduction:.1f}%\")\n",
    "\n",
    "# Limitations\n",
    "print(\"\\n5. LIMITATIONS\")\n",
    "print(\"-\" * 40)\n",
    "print(\"  - Computational overhead: LightGBM training and SHAP computation\")\n",
    "print(\"    add latency compared to statistical-test-based detectors\")\n",
    "print(\"  - Window size sensitivity: Performance depends on reference/current\")\n",
    "print(\"    window sizes (500/200 in current configuration)\")\n",
    "print(\"  - Gradual drift detection: Slower transitions may require larger\")\n",
    "print(\"    current windows to accumulate sufficient distributional difference\")\n",
    "print(\"  - Permutation test cost: B=100 permutations multiply the\")\n",
    "print(\"    classifier training time at each monitoring step\")\n",
    "\n",
    "# Threats to validity\n",
    "print(\"\\n6. THREATS TO VALIDITY\")\n",
    "print(\"-\" * 40)\n",
    "print(\"  Internal: Synthetic data may not capture all real-world drift patterns\")\n",
    "print(\"  External: Results may vary with different hyperparameter configurations\")\n",
    "print(\"  Construct: F1-score tolerance window (500 samples) affects metric values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f35253",
   "metadata": {},
   "source": [
    "## Section 16: Generate LaTeX Tables for Thesis\n",
    "\n",
    "Export all results as formatted LaTeX tables ready for inclusion in the thesis document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be88c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════\n",
    "# Generate LaTeX Tables for Thesis\n",
    "# ═══════════════════════════════════════════════════════════\n",
    "\n",
    "latex_output = []\n",
    "\n",
    "# Table 1: Experiment 1 - Synthetic Dataset Results\n",
    "print(\"=\" * 70)\n",
    "print(\"TABLE 1: Synthetic Dataset Detection Performance\")\n",
    "print(\"=\" * 70)\n",
    "table1 = r\"\"\"\n",
    "\\begin{table}[!htbp]\n",
    "\\centering\n",
    "\\caption{Experiment 1: Detection Performance Across Temporal Drift Patterns}\n",
    "\\label{tab:exp1-temporal}\n",
    "\\begin{tabular}{lcccccc}\n",
    "\\hline\n",
    "\\textbf{Drift Type} & \\multicolumn{2}{c}{\\textbf{F1-Score}} & \\multicolumn{2}{c}{\\textbf{Detection Delay}} & \\multicolumn{2}{c}{\\textbf{False Positives}} \\\\\n",
    " & EADD & D3 & EADD & D3 & EADD & D3 \\\\\n",
    "\\hline\n",
    "\"\"\"\n",
    "for dt in DRIFT_TYPES:\n",
    "    eadd = df_exp1[(df_exp1['drift_type'] == dt) & (df_exp1['detector'] == 'EADD')]\n",
    "    d3 = df_exp1[(df_exp1['drift_type'] == dt) & (df_exp1['detector'] == 'D3')]\n",
    "    if not eadd.empty and not d3.empty:\n",
    "        table1 += (f\"{dt.title()} & \"\n",
    "                   f\"{eadd['mean_f1'].values[0]:.3f} & {d3['mean_f1'].values[0]:.3f} & \"\n",
    "                   f\"{eadd['mean_delay'].values[0]:.0f} & {d3['mean_delay'].values[0]:.0f} & \"\n",
    "                   f\"{eadd['mean_false_positives'].values[0]:.1f} & {d3['mean_false_positives'].values[0]:.1f} \\\\\\\\\\n\")\n",
    "table1 += r\"\"\"\\hline\n",
    "\\end{tabular}\n",
    "\\end{table}\"\"\"\n",
    "print(table1)\n",
    "latex_output.append(table1)\n",
    "\n",
    "# Table 2: Experiment 3 - Explainability Results\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"TABLE 2: SHAP-Based Root Cause Analysis Results\")\n",
    "print(\"=\"*70)\n",
    "table2 = r\"\"\"\n",
    "\\begin{table}[!htbp]\n",
    "\\centering\n",
    "\\caption{Experiment 3: SHAP-Based Root Cause Analysis Results}\n",
    "\\label{tab:exp3-shap}\n",
    "\\begin{tabular}{llccl}\n",
    "\\hline\n",
    "\\textbf{Scenario} & \\textbf{Ground Truth} & \\textbf{Top Feature} & \\textbf{Importance (\\%)} & \\textbf{Correct?} \\\\\n",
    "\\hline\n",
    "\"\"\"\n",
    "for r in exp3_results:\n",
    "    correct = r\"\\checkmark\" if r['correct_attribution'] else r\"$\\times$\"\n",
    "    table2 += (f\"{r['scenario'].title()} & {r['target']} & \"\n",
    "               f\"{r['top_feature']} & {r['top_importance_pct']:.1f} & {correct} \\\\\\\\\\n\")\n",
    "table2 += r\"\"\"\\hline\n",
    "\\end{tabular}\n",
    "\\end{table}\"\"\"\n",
    "print(table2)\n",
    "latex_output.append(table2)\n",
    "\n",
    "# Table 3: Experiment 4 - False Alarm Results\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"TABLE 3: False Alarm Rates on Stable Data Streams\")\n",
    "print(\"=\"*70)\n",
    "table3 = r\"\"\"\n",
    "\\begin{table}[!htbp]\n",
    "\\centering\n",
    "\\caption{Experiment 4: False Alarm Counts on Stable Data Streams (Mean $\\pm$ Std)}\n",
    "\\label{tab:exp4-false-alarms}\n",
    "\\begin{tabular}{lcc}\n",
    "\\hline\n",
    "\\textbf{Stream Type} & \\textbf{EADD} & \\textbf{D3 ($\\tau$=0.7)} \\\\\n",
    "\\hline\n",
    "\"\"\"\n",
    "for st in STABLE_TYPES:\n",
    "    eadd = df_exp4[(df_exp4['stream_type'] == st) & (df_exp4['detector'] == 'EADD')]\n",
    "    d3 = df_exp4[(df_exp4['stream_type'] == st) & (df_exp4['detector'] == 'D3(τ=0.7)')]\n",
    "    if not eadd.empty and not d3.empty:\n",
    "        table3 += (f\"{st.title()} & \"\n",
    "                   f\"{eadd['mean_false_alarms'].values[0]:.1f}$\\\\pm${eadd['std_false_alarms'].values[0]:.1f} & \"\n",
    "                   f\"{d3['mean_false_alarms'].values[0]:.1f}$\\\\pm${d3['std_false_alarms'].values[0]:.1f} \\\\\\\\\\n\")\n",
    "table3 += r\"\"\"\\hline\n",
    "\\end{tabular}\n",
    "\\end{table}\"\"\"\n",
    "print(table3)\n",
    "latex_output.append(table3)\n",
    "\n",
    "# Table 4: Statistical Test Results\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"TABLE 4: Statistical Hypothesis Test Results\")\n",
    "print(\"=\"*70)\n",
    "table4 = r\"\"\"\n",
    "\\begin{table}[!htbp]\n",
    "\\centering\n",
    "\\caption{Statistical Hypothesis Test Results ($\\alpha$ = 0.05)}\n",
    "\\label{tab:hypothesis-tests}\n",
    "\\begin{tabular}{llccp{4cm}}\n",
    "\\hline\n",
    "\\textbf{Hypothesis} & \\textbf{Test} & \\textbf{Statistic} & \\textbf{$p$-value} & \\textbf{Decision} \\\\\n",
    "\\hline\n",
    "\"\"\"\n",
    "for _, row in df_hyp.iterrows():\n",
    "    decision = \"Reject $H_0$\" if row['significant'] else \"Fail to reject $H_0$\"\n",
    "    table4 += (f\"{row['hypothesis']} & {row['test']} & \"\n",
    "               f\"{row['statistic']:.4f} & {row['p_value']:.4f} & {decision} \\\\\\\\\\n\")\n",
    "table4 += r\"\"\"\\hline\n",
    "\\end{tabular}\n",
    "\\end{table}\"\"\"\n",
    "print(table4)\n",
    "latex_output.append(table4)\n",
    "\n",
    "# Save all LaTeX tables to file\n",
    "with open(os.path.join(FIGURES_DIR, \"latex_tables.tex\"), \"w\") as f:\n",
    "    for table in latex_output:\n",
    "        f.write(table + \"\\n\\n\")\n",
    "print(f\"\\nAll LaTeX tables saved to experiments/figures/latex_tables.tex\")\n",
    "\n",
    "# Save all results CSVs\n",
    "print(\"\\nAll experiment CSV files:\")\n",
    "for f in os.listdir(RESULTS_DIR):\n",
    "    if f.endswith('.csv'):\n",
    "        print(f\"  {os.path.join(RESULTS_DIR, f)}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
